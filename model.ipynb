{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7397ee48-93f5-4d50-9f1e-cf89240f2aa8",
   "metadata": {},
   "source": [
    "# Training an AI to play snake game \n",
    "\n",
    "Train a neural network to play snake game using reinforcement learning with Deep Q Network and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c493f9-c914-42cb-9934-24719c96356d",
   "metadata": {},
   "source": [
    "## Steps to Implement a DQN\n",
    "1. Import dependencies\n",
    "2. Define hyperparameters\n",
    "3. Load Environment\n",
    "4. Define Q-Network and the Target Q-Network\n",
    "5. Define experience replay buffer\n",
    "6. Define loss function\n",
    "7. Define agent learn function\n",
    "8. Define training loop\n",
    "9. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518b3364-801f-43f1-be43-e7e4107046e6",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7376723a-4410-4aee-b59f-8db84458b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import deque, namedtuple\n",
    "from environment import Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77ba46-cd27-425d-bf41-fac67bcbdcaa",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c31773-7815-40eb-a385-74fcfd05f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100000\n",
    "MINI_BATCH_SIZE = 1000\n",
    "NUM_STEPS_FOR_UPDATE = 4\n",
    "GAMMA = 0.995\n",
    "ALPHA = 1e-3\n",
    "TAU = 0.01\n",
    "EPS_DECAY = 0.9995\n",
    "EPS_MIN = 0.01\n",
    "NUM_EPISODES = 10000\n",
    "MAX_TIMESTEPS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c05860-8d37-4d1b-aa65-de6a316f937f",
   "metadata": {},
   "source": [
    "### Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3783934c-ad68-4e9e-875d-5c14c8963e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "state_size = env.state_size\n",
    "number_of_actions = env.number_of_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96924684-8315-4eca-b90a-de9e97821aa2",
   "metadata": {},
   "source": [
    "### Define Q-Network and the Target Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f227de97-c4ed-4603-8022-54c7a97b4659",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, number_of_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        return y\n",
    "\n",
    "class TargetQNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(state_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, number_of_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a978d5d-99ad-4082-85cc-d2cfd82ff255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "TargetQNetwork(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "q_network = QNetwork().to(device)\n",
    "target_q_network = TargetQNetwork().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(q_network.parameters(), lr=ALPHA)\n",
    "\n",
    "print(q_network)\n",
    "print(target_q_network)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4bebb5-32cb-4bb4-b0d4-51147b6d2cd3",
   "metadata": {},
   "source": [
    "### Define experience replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0c82de-f685-41ca-9bdb-0b5f03434ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4234ff7-b441-41e4-aede-14fe81974192",
   "metadata": {},
   "source": [
    "### Define Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "292062ab-6dc0-452f-b18d-3823c7f9f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiences is the mini batch of [states, actions, rewards, next_state, done]\n",
    "def loss_fn(experiences):\n",
    "    states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "    actions = torch.unsqueeze(actions, 1)\n",
    "    \n",
    "    max_qsa, _ = torch.max(target_q_network(next_states), dim=1)\n",
    "    \n",
    "    y = rewards + (1 - dones) * (GAMMA) * max_qsa\n",
    "    \n",
    "    pred = torch.gather(q_network(states), 1, actions)\n",
    "    pred = torch.squeeze(pred)\n",
    "    \n",
    "    MSE = nn.MSELoss()\n",
    "\n",
    "    return MSE(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c7c3ce-2e14-414c-9755-ab9c57342f8a",
   "metadata": {},
   "source": [
    "### Define Learning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e5552f2-de19-46f7-91b4-157d0d16e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiences is the mini batch of [states, actions, rewards, next_state, done]\n",
    "def agent_learn(experiences):\n",
    "    # Gradient Descent\n",
    "\n",
    "    loss = loss_fn(experiences)\n",
    "\n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Soft Update\n",
    "    for target_param, local_param in zip(target_q_network.parameters(), q_network.parameters()):\n",
    "        target_param.data.copy_(TAU*local_param.data + (1.0-TAU)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5f3b9-2053-4cb3-a069-0c32b81c496c",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34e279a5-700f-4576-8734-abd22f62e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps-greedy policy\n",
    "def get_action(state, epsilon):\n",
    "    prob = random.random()\n",
    "    \n",
    "    if (prob <= epsilon):\n",
    "        return random.randint(0, 2)\n",
    "    else:\n",
    "        mx = 0\n",
    "        with torch.no_grad():\n",
    "            mx = np.argmax(q_network(state).numpy()[0])\n",
    "        return mx\n",
    "\n",
    "# sample mini batch\n",
    "def get_experiences(memory_buffer):\n",
    "    experiences = random.sample(memory_buffer, k=MINI_BATCH_SIZE)\n",
    "    states = torch.from_numpy(np.array([e.state for e in experiences])).float()\n",
    "    actions = torch.from_numpy(np.array([e.action for e in experiences])).long()\n",
    "    rewards = torch.from_numpy(np.array([e.reward for e in experiences])).float()\n",
    "    next_states = torch.from_numpy(np.array([e.next_state for e in experiences])).float()\n",
    "    done_vals = torch.from_numpy(np.array([e.done for e in experiences])).int()\n",
    "    \n",
    "    return (states, actions, rewards, next_states, done_vals)\n",
    "\n",
    "def train():\n",
    "    start = time.time()\n",
    "    total_point_history = []\n",
    "    num_p_av = 100\n",
    "    epsilon = 1.0\n",
    "    memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "    target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    for i in range(NUM_EPISODES):\n",
    "        state = env.reset()\n",
    "        total_points = 0\n",
    "\n",
    "        for t in range(MAX_TIMESTEPS):\n",
    "            state_tensor = torch.tensor([state])\n",
    "            state_tensor = state_tensor.float()\n",
    "            action = get_action(state_tensor, epsilon)\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            memory_buffer.append(Experience(state, action, reward, next_state, done))\n",
    "\n",
    "            state = next_state\n",
    "            total_points += reward\n",
    "\n",
    "            if (len(memory_buffer) >= MINI_BATCH_SIZE and t % NUM_STEPS_FOR_UPDATE == 0):\n",
    "                experiences = get_experiences(memory_buffer)\n",
    "                agent_learn(experiences)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_point_history.append(total_points)\n",
    "\n",
    "        av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "        # Update the ε value\n",
    "        epsilon = max(EPS_MIN, epsilon * EPS_DECAY)\n",
    "        \n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "    \n",
    "        if (i+1) % num_p_av == 0:\n",
    "            print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "    \n",
    "    tot_time = time.time() - start\n",
    "    \n",
    "    print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f09721b-a036-4a14-a507-67f24ff801e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 | Total point average of the last 100 episodes: -8.90\n",
      "Episode 200 | Total point average of the last 100 episodes: -8.95\n",
      "Episode 300 | Total point average of the last 100 episodes: -8.20\n",
      "Episode 400 | Total point average of the last 100 episodes: -7.25\n",
      "Episode 500 | Total point average of the last 100 episodes: -7.35\n",
      "Episode 600 | Total point average of the last 100 episodes: -6.50\n",
      "Episode 700 | Total point average of the last 100 episodes: -5.95\n",
      "Episode 800 | Total point average of the last 100 episodes: -4.35\n",
      "Episode 900 | Total point average of the last 100 episodes: -5.25\n",
      "Episode 1000 | Total point average of the last 100 episodes: -4.15\n",
      "Episode 1100 | Total point average of the last 100 episodes: -4.30\n",
      "Episode 1200 | Total point average of the last 100 episodes: -3.30\n",
      "Episode 1300 | Total point average of the last 100 episodes: -2.30\n",
      "Episode 1400 | Total point average of the last 100 episodes: -2.00\n",
      "Episode 1500 | Total point average of the last 100 episodes: -2.00\n",
      "Episode 1600 | Total point average of the last 100 episodes: -0.80\n",
      "Episode 1700 | Total point average of the last 100 episodes: -0.45\n",
      "Episode 1800 | Total point average of the last 100 episodes: 1.155\n",
      "Episode 1900 | Total point average of the last 100 episodes: 1.80\n",
      "Episode 2000 | Total point average of the last 100 episodes: 1.45\n",
      "Episode 2100 | Total point average of the last 100 episodes: 4.70\n",
      "Episode 2200 | Total point average of the last 100 episodes: 1.10\n",
      "Episode 2300 | Total point average of the last 100 episodes: 5.90\n",
      "Episode 2400 | Total point average of the last 100 episodes: 5.50\n",
      "Episode 2500 | Total point average of the last 100 episodes: 5.20\n",
      "Episode 2600 | Total point average of the last 100 episodes: 6.00\n",
      "Episode 2700 | Total point average of the last 100 episodes: 8.50\n",
      "Episode 2800 | Total point average of the last 100 episodes: 9.550\n",
      "Episode 2900 | Total point average of the last 100 episodes: 10.55\n",
      "Episode 3000 | Total point average of the last 100 episodes: 13.20\n",
      "Episode 3100 | Total point average of the last 100 episodes: 10.60\n",
      "Episode 3200 | Total point average of the last 100 episodes: 9.655\n",
      "Episode 3300 | Total point average of the last 100 episodes: 12.05\n",
      "Episode 3400 | Total point average of the last 100 episodes: 14.15\n",
      "Episode 3500 | Total point average of the last 100 episodes: 13.25\n",
      "Episode 3600 | Total point average of the last 100 episodes: 16.50\n",
      "Episode 3700 | Total point average of the last 100 episodes: 20.25\n",
      "Episode 3800 | Total point average of the last 100 episodes: 16.30\n",
      "Episode 3900 | Total point average of the last 100 episodes: 16.10\n",
      "Episode 4000 | Total point average of the last 100 episodes: 19.10\n",
      "Episode 4100 | Total point average of the last 100 episodes: 21.10\n",
      "Episode 4200 | Total point average of the last 100 episodes: 21.10\n",
      "Episode 4300 | Total point average of the last 100 episodes: 20.90\n",
      "Episode 4400 | Total point average of the last 100 episodes: 24.55\n",
      "Episode 4500 | Total point average of the last 100 episodes: 28.65\n",
      "Episode 4600 | Total point average of the last 100 episodes: 27.20\n",
      "Episode 4700 | Total point average of the last 100 episodes: 25.75\n",
      "Episode 4800 | Total point average of the last 100 episodes: 30.15\n",
      "Episode 4900 | Total point average of the last 100 episodes: 30.60\n",
      "Episode 5000 | Total point average of the last 100 episodes: 34.50\n",
      "Episode 5100 | Total point average of the last 100 episodes: 32.75\n",
      "Episode 5200 | Total point average of the last 100 episodes: 27.80\n",
      "Episode 5300 | Total point average of the last 100 episodes: 35.95\n",
      "Episode 5400 | Total point average of the last 100 episodes: 35.55\n",
      "Episode 5500 | Total point average of the last 100 episodes: 33.45\n",
      "Episode 5600 | Total point average of the last 100 episodes: 38.00\n",
      "Episode 5700 | Total point average of the last 100 episodes: 38.30\n",
      "Episode 5800 | Total point average of the last 100 episodes: 40.40\n",
      "Episode 5900 | Total point average of the last 100 episodes: 42.45\n",
      "Episode 6000 | Total point average of the last 100 episodes: 44.55\n",
      "Episode 6100 | Total point average of the last 100 episodes: 43.20\n",
      "Episode 6200 | Total point average of the last 100 episodes: 41.50\n",
      "Episode 6300 | Total point average of the last 100 episodes: 41.40\n",
      "Episode 6400 | Total point average of the last 100 episodes: 50.50\n",
      "Episode 6500 | Total point average of the last 100 episodes: 46.30\n",
      "Episode 6600 | Total point average of the last 100 episodes: 46.55\n",
      "Episode 6700 | Total point average of the last 100 episodes: 49.05\n",
      "Episode 6800 | Total point average of the last 100 episodes: 47.50\n",
      "Episode 6900 | Total point average of the last 100 episodes: 53.35\n",
      "Episode 7000 | Total point average of the last 100 episodes: 50.35\n",
      "Episode 7100 | Total point average of the last 100 episodes: 47.80\n",
      "Episode 7200 | Total point average of the last 100 episodes: 54.25\n",
      "Episode 7300 | Total point average of the last 100 episodes: 55.25\n",
      "Episode 7400 | Total point average of the last 100 episodes: 53.40\n",
      "Episode 7500 | Total point average of the last 100 episodes: 56.35\n",
      "Episode 7600 | Total point average of the last 100 episodes: 53.35\n",
      "Episode 7700 | Total point average of the last 100 episodes: 57.10\n",
      "Episode 7800 | Total point average of the last 100 episodes: 57.85\n",
      "Episode 7900 | Total point average of the last 100 episodes: 55.80\n",
      "Episode 8000 | Total point average of the last 100 episodes: 51.75\n",
      "Episode 8100 | Total point average of the last 100 episodes: 62.45\n",
      "Episode 8200 | Total point average of the last 100 episodes: 61.10\n",
      "Episode 8300 | Total point average of the last 100 episodes: 67.85\n",
      "Episode 8400 | Total point average of the last 100 episodes: 65.30\n",
      "Episode 8500 | Total point average of the last 100 episodes: 62.60\n",
      "Episode 8600 | Total point average of the last 100 episodes: 63.05\n",
      "Episode 8700 | Total point average of the last 100 episodes: 63.55\n",
      "Episode 8800 | Total point average of the last 100 episodes: 64.55\n",
      "Episode 8900 | Total point average of the last 100 episodes: 63.15\n",
      "Episode 9000 | Total point average of the last 100 episodes: 69.10\n",
      "Episode 9100 | Total point average of the last 100 episodes: 64.95\n",
      "Episode 9200 | Total point average of the last 100 episodes: 67.40\n",
      "Episode 9300 | Total point average of the last 100 episodes: 68.80\n",
      "Episode 9400 | Total point average of the last 100 episodes: 65.10\n",
      "Episode 9500 | Total point average of the last 100 episodes: 58.15\n",
      "Episode 9600 | Total point average of the last 100 episodes: 59.35\n",
      "Episode 9700 | Total point average of the last 100 episodes: 69.50\n",
      "Episode 9800 | Total point average of the last 100 episodes: 63.70\n",
      "Episode 9900 | Total point average of the last 100 episodes: 69.90\n",
      "Episode 10000 | Total point average of the last 100 episodes: 66.40\n",
      "\n",
      "Total Runtime: 2008.28 s (33.47 min)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a0a182-9dd4-4577-a0e4-60c854a47bab",
   "metadata": {},
   "source": [
    "### Dumb Snake In Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d07449-34d1-4d20-868c-c69a427791f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    from IPython.display import clear_output\n",
    "    state = env.reset()\n",
    "    total_points = 0\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.5)\n",
    "    while (True):\n",
    "        state_tensor = torch.tensor([state])\n",
    "        state_tensor = state_tensor.float()\n",
    "        action = 0\n",
    "        with torch.no_grad():\n",
    "            action = np.argmax(q_network(state_tensor).numpy()[0])\n",
    "    \n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "        total_points += reward\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        if done:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7392f70a-e906-415a-a3f1-8658812a762e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....****\n",
      "..***..*\n",
      "..*....*\n",
      "..*....*\n",
      "..*....*\n",
      ".@*...**\n",
      "..*..***\n",
      "..****o*\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68473e-1a91-49f7-8e71-5f755d6711f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
